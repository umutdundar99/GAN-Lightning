dataset:
  name: celeba
  data_dir: data/celeba
  num_workers: 12
  normalize: true
  download: true
  shuffle: true
  train: true
  pin_memory: true
  batch_size: 128
  num_classes: 40
training_params:
  n_epochs: 200
  display_step: 500
  device_num: [0]
  accelerator: gpu
  weight_init_name: normal
  checkpoint_dir: checkpoints/
  monitor: train
  model:
    input_dim: 128
    architecture: DeepConvGAN
    input_size: 64 # set it to 28 for mnist and 32 for cifar10
    pretrained: false
  loss:
    discriminator_loss: BasicDiscLoss
    generator_loss: BasicGenLoss
  optimizer:
    optimizer_name: adam
    lr: 0.0002
    scheduler:
      scheduler_name: StepLR
      step_size: 25
      gamma: 1.0
logger:
  tool: mlflow
  experiment_name: DeepConv
  run_name: DeepConv
  tracking_uri: localhost:5000
  tags: experimental
  log_model: all
